{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Fashion Dataset CNN with TensorFlow\n",
        "\n",
        "Jackson Eshbaugh &bull; CS 424 &bull; Spring 2025\n",
        "\n",
        "**Abstract**: This notebook trains and tunes a convolutional neural network on Fashion-MNIST using `KerasTuner` with TensorBoard logging, early stopping, and checkpoint saving."
      ],
      "metadata": {
        "id": "YJF7WDf4OCop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "6y_S_SNRe4iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "0jFDwlqgGmcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection: Run Pretrained Model or Tune Model Again?\n",
        "\n",
        "**If you run the pretrained model, ensure**:\n",
        "\n",
        "1. `model.weights.h5` is uploaded into the same folder as this file,\n",
        "2. `hyperparams.json` is uploaded into the same folder as this file, and\n",
        "3. `logs/trial_06/06/execution0` is uploaded into the same folder as this file.\n",
        "\n",
        "All of these can be found within the GitHub repository."
      ],
      "metadata": {
        "id": "fc_q8RB4Jq-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_tuning = \"Load Pretrained Model\"  #@param [\"Run Hyperparameter Tuning\", \"Load Pretrained Model\"]\n",
        "run_tuning = run_tuning == \"Run Hyperparameter Tuning\""
      ],
      "metadata": {
        "id": "z4OMKlJ8mzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import the Dataset and Preprocess the Data\n",
        "\n",
        "- Normalize pixel values to range [0, 1]\n",
        "- Add the channel value (required by TF for CNNs)"
      ],
      "metadata": {
        "id": "yH6i3ybnxlFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train_pre, y_train_pre), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Print the shape of the training images and labels\n",
        "\n",
        "print(\"Training images shape:\", X_train_pre.shape)\n",
        "print(\"Training labels shape:\", y_train_pre.shape)\n",
        "print(\"Test images shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)\n",
        "\n",
        "# Get some size data about the image\n",
        "\n",
        "print(\"First training image shape:\", X_train_pre[0].shape)\n",
        "\n",
        "# Normalize pixel values to [0, 1] and reshape for CNN\n",
        "\n",
        "X_train_pre = X_train_pre.astype('float32') / 255.0\n",
        "X_test  = X_test.astype('float32') / 255.0\n",
        "X_train_pre = X_train_pre[..., tf.newaxis]  # shape is updated: (num_samples, 28, 28, 1)\n",
        "X_test  = X_test[..., tf.newaxis]\n",
        "\n",
        "# Get validation set\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_pre, y_train_pre, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Get the same size data about the image again; now we have the channel value\n",
        "\n",
        "print(\"First training image shape:\", X_train[0].shape)\n",
        "\n",
        "# Sizes\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "STwJeN7uxmto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just Run the Model\n",
        "\n",
        "Below, you can import the precompiled model and evaluate it (by selecting \"Load Pretrained Model\" from the dropdown above)"
      ],
      "metadata": {
        "id": "o1jDUJHqsmFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not run_tuning:\n",
        "\n",
        "  fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "  (_, _), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  X_test = X_test.astype(\"float32\") / 255.0\n",
        "  X_test = X_test[..., tf.newaxis]  # Add channel dimension\n",
        "\n",
        "\n",
        "  # Load saved hyperparameters from file\n",
        "  import json\n",
        "  filename = 'model.weights.h5'\n",
        "  hyperparams_file = 'hyperparams.json'\n",
        "\n",
        "  from tensorflow import keras\n",
        "  from tensorflow.keras import layers\n",
        "\n",
        "  with open(hyperparams_file, 'r') as f:\n",
        "      best_hp = json.load(f)\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      keras.Input(shape=(28, 28, 1)),\n",
        "\n",
        "      layers.Conv2D(best_hp['conv1_filters'], kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D(),\n",
        "\n",
        "      layers.Conv2D(best_hp['conv2_filters'], kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D(),\n",
        "\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(best_hp['dense_units'], activation='relu'),\n",
        "      layers.Dropout(best_hp['dropout_rate']),\n",
        "      layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=best_hp['learning_rate']),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  model.load_weights(filename)  # Load pretrained weights\n",
        "\n",
        "  test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "  print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "NJc5uMQ9sfkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not run_tuning:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "qMoz62fRxOze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the Model\n",
        "\n",
        "We propose a model that uses 2D convolutional layers, which capture more spatial information—and are super useful for image processing. We control for overfitting by using a dropout layer before making the final prediction. Pooling allows us to reduce the dimensionality of the images. Meanwhile, we increase the number of filters. These two actions go hand in hand and help us to recognize richer information in the images.\n",
        "\n",
        "We use the categorical crossentropy loss function to optimize the model, with the `adam` optimizer."
      ],
      "metadata": {
        "id": "QPeyZ1_1Pm7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "\n",
        "  # Get the hyperparameters we're tuning.\n",
        "\n",
        "  filters_1 = hp.Choice('conv1_filters', [32, 64])\n",
        "  filters_2 = hp.Choice('conv2_filters', [64, 128])\n",
        "  dense_units = hp.Int('dense_units', 64, 256, step=64)\n",
        "  dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "  lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
        "\n",
        "  model = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "\n",
        "    layers.Conv2D(filters_1, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Conv2D(filters_2, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(dense_units, activation='relu'),\n",
        "    layers.Dropout(dropout_rate),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(\n",
        "          hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
        "      ),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "RCk20JcNNIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Hyperparameter Tuning\n",
        "\n",
        "We now tune the model using **Bayesian Optimization**. To support trial-specific logging and checkpointing, we define a custom subclass of `BayesianOptimization` that allows us to assign **unique callbacks** (such as TensorBoard and ModelCheckpoint) to each individual trial. This ensures that we save the best model weights and learning curves separately for each hyperparameter configuration. We also setup callbacks for early stopping, saving a model checkpoint, and for TensorBoard logging within the `run_trial` method.\n",
        "\n",
        "Within the `build_model` function, we tune the following five hyperparameters:\n",
        "\n",
        "1. **`conv1_filters`** – The number of filters in the first convolutional layer. This controls how much low-level detail (e.g., edges, corners) the model can extract from input images.\n",
        "\n",
        "2. **`conv2_filters`** – The number of filters in the second convolutional layer. These deeper filters typically learn more complex patterns such as textures and shapes.\n",
        "\n",
        "3. **`dense_units`** – The number of neurons in the dense (fully connected) layer before the output. This layer helps the model combine features to make final predictions, so tuning its size affects model capacity.\n",
        "\n",
        "4. **`dropout_rate`** – The probability of \"dropping\" a neuron during training. Dropout is a form of regularization that helps prevent overfitting, especially on relatively small datasets like Fashion-MNIST.\n",
        "\n",
        "5. **`learning_rate`** – The size of the step taken by the optimizer during each parameter update. This is a critical parameter for controlling convergence speed and training stability."
      ],
      "metadata": {
        "id": "q3lwbQelUmtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CallbackTuner(kt.BayesianOptimization):\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        model = self.hypermodel.build(trial.hyperparameters)\n",
        "\n",
        "        trial_id = trial.trial_id\n",
        "\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=f\"checkpoints/trial_{trial_id}_best.weights.h5\",\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=3,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            keras.callbacks.TensorBoard(\n",
        "                log_dir=f\"logs/trial_{trial_id}\",\n",
        "                histogram_freq=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return super().run_trial(trial, *args, callbacks=callbacks, **kwargs)"
      ],
      "metadata": {
        "id": "BiNRgZEoaBjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tuning:\n",
        "  tuner = CallbackTuner(\n",
        "      hypermodel=build_model,\n",
        "      objective='val_accuracy',\n",
        "      max_trials=10,\n",
        "      directory='tuner_dir',\n",
        "      project_name='fashion_mnist'\n",
        "  )\n",
        "\n",
        "  tuner.search(\n",
        "      X_train, y_train,\n",
        "      epochs=20,\n",
        "      validation_data=(X_val, y_val),\n",
        "  )"
      ],
      "metadata": {
        "id": "NDxmYtV5UnAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tuning:\n",
        "  import json\n",
        "\n",
        "  best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "  with open(\"best_hp.json\", \"w\") as f:\n",
        "      json.dump(best_hp.values, f)"
      ],
      "metadata": {
        "id": "-sYhXqJ-n38S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the TensorBoard for these trials here."
      ],
      "metadata": {
        "id": "BLsaLa_kU4mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "GmDGGloyU4ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation\n",
        "\n",
        "Now that we've optimized the hyperparameters, we pick the best model out and run it."
      ],
      "metadata": {
        "id": "EI-T-YQ4VAeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_best_model(hp):\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(hp.get('conv1_filters'), 3, activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(hp.get('conv2_filters'), 3, activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(hp.get('dense_units'), activation='relu'),\n",
        "        layers.Dropout(hp.get('dropout_rate')),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "PZEfoMsDTX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tuning:\n",
        "  best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "  best_trial_id = tuner.oracle.get_best_trials(1)[0].trial_id\n",
        "\n",
        "  model = build_best_model(best_hp)\n",
        "  weights_path = f\"checkpoints/trial_{best_trial_id}_best.weights.h5\"\n",
        "  model.load_weights(weights_path)\n",
        "  best_trial_id"
      ],
      "metadata": {
        "id": "foX7eyZfU9oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trial 6 performed the best—let's view its learning curves using TensorBoard."
      ],
      "metadata": {
        "id": "AUEMYxy2wc1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/trial_06"
      ],
      "metadata": {
        "id": "KBj0LA9Dv5O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tuning:\n",
        "  test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "  print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "OtprTtl7UE0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "The best model achieved a test accuracy of $ 92.52\\% $, and the validiation set plateaued at around $ 92.91\\% $. Although it didn't reach the $ 98\\% $ we were (informally) aiming for, this model demonstrates excellence in clasifying the MNIST Fashion dataset.\n",
        "\n",
        "In a second iteration, I would consider:\n",
        "\n",
        "- adding a third convolutional block,\n",
        "- using batch normalization,\n",
        "- increasing the number of epochs (maintaining early stopping), and\n",
        "- expanding the search space\n",
        "  - tune batch size, kernal size, or even run more trials\n",
        "\n",
        "However, the current model generalizes quite well and meets the core objectives of the assignment: hyperparameter tuning, checkpointing, early stopping, and reproducible evaluation."
      ],
      "metadata": {
        "id": "C5Q8i-rsWhnN"
      }
    }
  ]
}